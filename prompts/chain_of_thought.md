# Chain-of-Thought (CoT) Prompts for ML Reasoning

These prompts are designed to test LLMs' reasoning abilities by encouraging them to "think aloud" — breaking down multi-step problems or explaining complex logic in the ML workflow.

---

### Prompt 1
> You observe that your model performs well on training and validation data, but poorly on test data. Walk through, step-by-step, how you'd diagnose this issue.

---

### Prompt 2
> A classification model has high precision but low recall. Think aloud through why this might be happening and what steps you’d take to adjust it.

---

### Prompt 3
> You're tuning a random forest model. List each hyperparameter you’d consider, and explain what effect it has on model performance.

---

### Prompt 4
> An ML system that worked perfectly in simulation fails after deployment. Walk through how you'd identify the causes, starting from monitoring to retraining.

---

### Prompt 5
> You need to choose between logistic regression, decision trees, and neural networks for a healthcare prediction task. Think aloud as you weigh the pros and cons for interpretability, performance, and resource constraints.

---

### Prompt 6
> You discover your ML model performs significantly better on one category of data than others. What hypotheses might explain this, and how would you test them?
